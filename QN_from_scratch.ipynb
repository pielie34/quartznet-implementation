{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "QN_from_scratch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "1WhQDK7gnu1SExB7Hn2SGDzH1vEzru9tP",
      "authorship_tag": "ABX9TyNjvMkDLjFrDlpC1n2WJzx9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pielie34/quartznet-implementation/blob/main/QN_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install youtokentome\n",
        "!pip install pyyaml\n",
        "!pip install easydict\n",
        "!pip install wandb\n",
        "!pip install torchaudio\n",
        "!pip install librosa\n",
        "!pip install python-Levenshtein\n",
        "!pip install audiomentations\n",
        "!pip install pytorch_warmup"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3nqHU2nd7rAk",
        "outputId": "ee6d3ec0-24c5-4139-f0b1-d86b2ff2fba3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: youtokentome in /usr/local/lib/python3.7/dist-packages (1.0.6)\n",
            "Requirement already satisfied: Click>=7.0 in /usr/local/lib/python3.7/dist-packages (from youtokentome) (7.1.2)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (3.13)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: easydict in /usr/local/lib/python3.7/dist-packages (1.9)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.7/dist-packages (0.12.17)\n",
            "Requirement already satisfied: pathtools in /usr/local/lib/python3.7/dist-packages (from wandb) (0.1.2)\n",
            "Requirement already satisfied: GitPython>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.1.27)\n",
            "Requirement already satisfied: protobuf<4.0dev,>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.17.3)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.7/dist-packages (from wandb) (1.2.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from wandb) (57.4.0)\n",
            "Requirement already satisfied: shortuuid>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.0.9)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.5.12)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.2)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (3.13)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.2.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.0.9)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (5.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2022.5.18.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.7/dist-packages (0.11.0+cu113)\n",
            "Requirement already satisfied: torch==1.11.0 in /usr/local/lib/python3.7/dist-packages (from torchaudio) (1.11.0+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.11.0->torchaudio) (4.2.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.7/dist-packages (0.8.1)\n",
            "Requirement already satisfied: audioread>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (2.1.9)\n",
            "Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (1.6.0)\n",
            "Requirement already satisfied: decorator>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (1.4.1)\n",
            "Requirement already satisfied: numba>=0.43.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (0.51.2)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.7/dist-packages (from librosa) (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (1.21.6)\n",
            "Requirement already satisfied: soundfile>=0.10.2 in /usr/local/lib/python3.7/dist-packages (from librosa) (0.10.3.post1)\n",
            "Requirement already satisfied: resampy>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from librosa) (0.2.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (21.3)\n",
            "Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (1.0.2)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.43.0->librosa) (0.34.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba>=0.43.0->librosa) (57.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->librosa) (3.0.9)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from pooch>=1.0->librosa) (2.23.0)\n",
            "Requirement already satisfied: appdirs>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from pooch>=1.0->librosa) (1.4.4)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2022.5.18.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (1.24.3)\n",
            "Requirement already satisfied: six>=1.3 in /usr/local/lib/python3.7/dist-packages (from resampy>=0.2.2->librosa) (1.15.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn!=0.19.0,>=0.14.0->librosa) (3.1.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.7/dist-packages (from soundfile>=0.10.2->librosa) (1.15.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.0->soundfile>=0.10.2->librosa) (2.21)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: python-Levenshtein in /usr/local/lib/python3.7/dist-packages (0.12.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from python-Levenshtein) (57.4.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: audiomentations in /usr/local/lib/python3.7/dist-packages (0.24.0)\n",
            "Requirement already satisfied: librosa<0.10.0,>0.7.2 in /usr/local/lib/python3.7/dist-packages (from audiomentations) (0.8.1)\n",
            "Requirement already satisfied: numpy>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from audiomentations) (1.21.6)\n",
            "Requirement already satisfied: scipy<2,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from audiomentations) (1.4.1)\n",
            "Requirement already satisfied: resampy>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from librosa<0.10.0,>0.7.2->audiomentations) (0.2.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from librosa<0.10.0,>0.7.2->audiomentations) (21.3)\n",
            "Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.7/dist-packages (from librosa<0.10.0,>0.7.2->audiomentations) (1.6.0)\n",
            "Requirement already satisfied: soundfile>=0.10.2 in /usr/local/lib/python3.7/dist-packages (from librosa<0.10.0,>0.7.2->audiomentations) (0.10.3.post1)\n",
            "Requirement already satisfied: decorator>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa<0.10.0,>0.7.2->audiomentations) (4.4.2)\n",
            "Requirement already satisfied: audioread>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa<0.10.0,>0.7.2->audiomentations) (2.1.9)\n",
            "Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from librosa<0.10.0,>0.7.2->audiomentations) (1.0.2)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.7/dist-packages (from librosa<0.10.0,>0.7.2->audiomentations) (1.1.0)\n",
            "Requirement already satisfied: numba>=0.43.0 in /usr/local/lib/python3.7/dist-packages (from librosa<0.10.0,>0.7.2->audiomentations) (0.51.2)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.43.0->librosa<0.10.0,>0.7.2->audiomentations) (0.34.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba>=0.43.0->librosa<0.10.0,>0.7.2->audiomentations) (57.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->librosa<0.10.0,>0.7.2->audiomentations) (3.0.9)\n",
            "Requirement already satisfied: appdirs>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from pooch>=1.0->librosa<0.10.0,>0.7.2->audiomentations) (1.4.4)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from pooch>=1.0->librosa<0.10.0,>0.7.2->audiomentations) (2.23.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa<0.10.0,>0.7.2->audiomentations) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa<0.10.0,>0.7.2->audiomentations) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa<0.10.0,>0.7.2->audiomentations) (2022.5.18.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa<0.10.0,>0.7.2->audiomentations) (1.24.3)\n",
            "Requirement already satisfied: six>=1.3 in /usr/local/lib/python3.7/dist-packages (from resampy>=0.2.2->librosa<0.10.0,>0.7.2->audiomentations) (1.15.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn!=0.19.0,>=0.14.0->librosa<0.10.0,>0.7.2->audiomentations) (3.1.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.7/dist-packages (from soundfile>=0.10.2->librosa<0.10.0,>0.7.2->audiomentations) (1.15.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.0->soundfile>=0.10.2->librosa<0.10.0,>0.7.2->audiomentations) (2.21)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pytorch_warmup in /usr/local/lib/python3.7/dist-packages (0.1.0)\n",
            "Requirement already satisfied: torch>=1.1 in /usr/local/lib/python3.7/dist-packages (from pytorch_warmup) (1.11.0+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.1->pytorch_warmup) (4.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!git clone --recursive https://github.com/parlance/ctcdecode.git\n",
        "#!cd ctcdecode && pip install ."
      ],
      "metadata": {
        "id": "fWn74J9sbYz6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import string\n",
        "import json\n",
        "import time\n",
        "import argparse\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict\n",
        "import math\n",
        "import argparse\n",
        "import importlib\n",
        "\n",
        "from torch import nn\n",
        "import torch\n",
        "from torch.utils import data\n",
        "from torch.utils.data import DataLoader, Subset, ConcatDataset\n",
        "from torch.utils.data.dataloader import default_collate\n",
        "import torchaudio\n",
        "from torchvision.transforms import Normalize\n",
        "\n",
        "import pytorch_warmup as warmup\n",
        "import youtokentome as yttm\n",
        "from audiomentations import TimeStretch, PitchShift, AddGaussianNoise\n",
        "from functools import partial\n",
        "import yaml\n",
        "from easydict import EasyDict as edict\n",
        "import wandb"
      ],
      "metadata": {
        "id": "-YT2LkoW0Udk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import Levenshtein as Lev"
      ],
      "metadata": {
        "id": "LjkT8i-_MEV_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data"
      ],
      "metadata": {
        "id": "1zCAA3LpzNmD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset"
      ],
      "metadata": {
        "id": "arufonFo1Rof"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LibriDataset(torchaudio.datasets.LIBRISPEECH):\n",
        "    def __init__(self, transforms, *args, **kwargs):\n",
        "        if kwargs.get('download', False):\n",
        "            os.makedirs(kwargs['root'], exist_ok=True)\n",
        "        super(LibriDataset, self).__init__(*args, **kwargs)\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        audio, sample_rate, text, _, _, _ = super().__getitem__(idx)\n",
        "        return self.transforms({'audio' : audio, 'text': text, 'sample_rate': sample_rate})\n",
        "\n",
        "    def get_text(self, idx):\n",
        "        fileid = self._walker[idx]\n",
        "        speaker_id, chapter_id, utterance_id = fileid.split(\"-\")\n",
        "\n",
        "        file_text = speaker_id + \"-\" + chapter_id + self._ext_txt\n",
        "        file_text = os.path.join(self._path, speaker_id, chapter_id, file_text)\n",
        "\n",
        "        fileid_audio = speaker_id + \"-\" + chapter_id + \"-\" + utterance_id\n",
        "        file_audio = fileid_audio + self._ext_audio\n",
        "\n",
        "        # Load text\n",
        "        with open(file_text) as ft:\n",
        "            for line in ft:\n",
        "                fileid_text, utterance = line.strip().split(\" \", 1)\n",
        "                if fileid_audio == fileid_text:\n",
        "                    break\n",
        "            else:\n",
        "                # Translation not found\n",
        "                raise FileNotFoundError(\"Translation not found for \" + fileid_audio)\n",
        "\n",
        "        return self.transforms({'text' : utterance})['text']\n",
        "\n",
        "\n",
        "def get_dataset(config, transforms=lambda x: x, part='train'):\n",
        "    if part == 'train':\n",
        "        dataset = LibriDataset(root='DB/LibriSpeech', url='train-clean-100', download=True, transforms=transforms)\n",
        "        return dataset\n",
        "    elif part == 'val':\n",
        "        dataset = LibriDataset(root='DB/LibriSpeech', url='dev-clean', download=True, transforms=transforms)\n",
        "        return dataset\n",
        "    elif part == 'bpe':\n",
        "        dataset = LibriDataset(root='DB/LibriSpeech', url='train-clean-100', download=True, transforms=transforms)\n",
        "        indices = list(range(len(dataset)))\n",
        "        return dataset, indices\n",
        "    else:\n",
        "        raise ValueError('Unknown')"
      ],
      "metadata": {
        "id": "mPYt6Y3GzQJo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Collate"
      ],
      "metadata": {
        "id": "IkDP8IDt1VYE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def no_pad_collate(batch):\n",
        "    keys = batch[0].keys()\n",
        "    collated_batch = {key: [] for key in keys}\n",
        "    for key in keys:\n",
        "        items = [item[key] for item in batch]\n",
        "        collated_batch[key] = items\n",
        "    return collated_batch\n",
        "\n",
        "\n",
        "def gpu_collate(batch):\n",
        "    '''\n",
        "    Padds batch of variable length\n",
        "    note: it converts things ToTensor manually here since the ToTensor transform\n",
        "    assume it takes in images rather than arbitrary tensors.\n",
        "    '''\n",
        "    keys = batch[0].keys()\n",
        "    collated_batch = {key: [] for key in keys}\n",
        "    for key in keys:\n",
        "        items = [item[key] for item in batch]\n",
        "        if len(items[0]) < 2:\n",
        "            items = [item[None] for item in items]\n",
        "        items = torch.nn.utils.rnn.pad_sequence(items)\n",
        "        collated_batch[key] = items\n",
        "    return collated_batch\n",
        "\n",
        "\n",
        "def collate_fn(batch):\n",
        "    keys = batch[0].keys()\n",
        "    max_lengths = {key: 0 for key in keys}\n",
        "    collated_batch = {key: [] for key in keys}\n",
        "\n",
        "    # find out the max lengths\n",
        "    for row in batch:\n",
        "        for key in keys:\n",
        "            if not np.isscalar(row[key]):\n",
        "                max_lengths[key] = max(max_lengths[key], row[key].shape[-1])\n",
        "\n",
        "    # pad to the max lengths\n",
        "    for row in batch:\n",
        "        for key in keys:\n",
        "            if not np.isscalar(row[key]):\n",
        "                array = row[key]\n",
        "                dim = len(array.shape)\n",
        "                assert dim == 1 or dim == 2\n",
        "                if dim == 1:\n",
        "                    padded_array = np.pad(array, (0, max_lengths[key] - array.shape[-1]), mode='constant')\n",
        "                else:\n",
        "                    # padded_array = np.pad(array, ((0, max_lengths[key] - array.shape[0]), (0, 0)), mode='constant')\n",
        "                    padded_array = np.pad(array, ((0, 0), (0, max_lengths[key] - array.shape[-1])), mode='constant')\n",
        "                collated_batch[key].append(padded_array)\n",
        "            else:\n",
        "                collated_batch[key].append(row[key])\n",
        "\n",
        "    # use the default_collate to convert to tensors\n",
        "    for key in keys:\n",
        "        collated_batch[key] = default_collate(collated_batch[key])\n",
        "    return collated_batch"
      ],
      "metadata": {
        "id": "S2yLmqPp1W7N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transforms"
      ],
      "metadata": {
        "id": "F6jFvh2g0Qj5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PUNCTUATION = string.punctuation + '—–«»−…‑'\n",
        "\n",
        "\n",
        "class Compose(object):\n",
        "    \"\"\"Composes several transforms together.\"\"\"\n",
        "\n",
        "    def __init__(self, transforms):\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __call__(self, data):\n",
        "        for t in self.transforms:\n",
        "            try:\n",
        "              data = t(data)\n",
        "            except TypeError:\n",
        "              # audiomentation transform\n",
        "              data['audio'] = t(data['audio'], sample_rate=data['sample_rate'])\n",
        "        return data\n",
        "\n",
        "\n",
        "class AudioSqueeze:\n",
        "    def __call__(self, data):\n",
        "        data['audio'] = data['audio'].squeeze(0)\n",
        "        return data\n",
        "\n",
        "\n",
        "class BPEtexts:\n",
        "    def __init__(self, bpe, dropout_prob=0):\n",
        "        self.bpe = bpe\n",
        "        self.dropout_prob = dropout_prob\n",
        "\n",
        "    def __call__(self, data):\n",
        "        data['text'] = np.array(self.bpe.encode(data['text'], dropout_prob=self.dropout_prob))\n",
        "        return data\n",
        "\n",
        "\n",
        "class TextPreprocess:\n",
        "    def __call__(self, data):\n",
        "        data['text'] = data['text'].lower().strip().translate(str.maketrans('', '', PUNCTUATION))\n",
        "        return data\n",
        "\n",
        "\n",
        "class ToNumpy:\n",
        "    \"\"\"\n",
        "    Transform to make numpy array\n",
        "    \"\"\"\n",
        "    def __call__(self, data):\n",
        "        data['audio'] = np.array(data['audio'])\n",
        "        return data\n",
        "\n",
        "# on gpu:\n",
        "\n",
        "class ToGpu:\n",
        "    def __init__(self, device):\n",
        "        self.device = device\n",
        "\n",
        "    def __call__(self, data):\n",
        "        data = {k: [torch.from_numpy(np.array(item)).to(self.device) for item in v] for k, v in data.items()}\n",
        "        return data\n",
        "\n",
        "\n",
        "class AddLengths:\n",
        "    def __call__(self, data):\n",
        "        data['input_lengths'] = torch.tensor([item.shape[-1] for item in data['audio']]).to(data['audio'][0].device)\n",
        "        data['target_lengths'] = torch.tensor([item.shape[0] for item in data['text']]).to(data['audio'][0].device)\n",
        "        return data\n",
        "\n",
        "\n",
        "class Pad:\n",
        "    def __call__(self, data):\n",
        "        padded_batch = {}\n",
        "        for k, v in data.items():\n",
        "            if len(v[0].shape) < 2:\n",
        "                items = [item[..., None] for item in v]\n",
        "                padded_batch[k] = torch.nn.utils.rnn.pad_sequence(items, batch_first=True)[..., 0]\n",
        "            else:\n",
        "                items = [item.permute(1, 0) for item in v]\n",
        "                padded_batch[k] = torch.nn.utils.rnn.pad_sequence(items, batch_first=True).permute(0, 2, 1)\n",
        "        return padded_batch\n",
        "\n",
        "\n",
        "class MelSpectrogram(torchaudio.transforms.MelSpectrogram):\n",
        "    def forward(self, data):\n",
        "        for i in range(len(data['audio'])):\n",
        "            data['audio'][i] = super(MelSpectrogram, self).forward(data['audio'][i])\n",
        "        return data\n",
        "\n",
        "\n",
        "class NormalizedMelSpectrogram(torchaudio.transforms.MelSpectrogram):\n",
        "    def __init__(self, normalize=None, *args, **kwargs):\n",
        "        super(NormalizedMelSpectrogram, self).__init__(*args, **kwargs)\n",
        "        if normalize == 'to05':\n",
        "            self.normalize = Normalize([0.5], [0.5])\n",
        "        elif normalize == 'touniform':\n",
        "            self.normalize = lambda x: (x - torch.mean(x, dim=1, keepdim=True)) / (torch.std(x, dim=1, keepdim=True) + 1e-18)\n",
        "        else:\n",
        "            self.normalize = None\n",
        "\n",
        "\n",
        "    def forward(self, data):\n",
        "        for i in range(len(data['audio'])):\n",
        "            melsec = super(NormalizedMelSpectrogram, self).forward(data['audio'][i])\n",
        "            if self.normalize is not None:\n",
        "                logmelsec = torch.log(torch.clamp(melsec, min=1e-18))\n",
        "                melsec = self.normalize(logmelsec[None])[0]\n",
        "            data['audio'][i] = melsec\n",
        "        return data\n",
        "\n",
        "\n",
        "class MaskSpectrogram(object):\n",
        "    \"\"\"Masking a spectrogram aka SpecAugment.\"\"\"\n",
        "\n",
        "    def __init__(self, frequency_mask_max_percentage=0.3, time_mask_max_percentage=0.1, probability=1.0):\n",
        "        self.frequency_mask_probability = frequency_mask_max_percentage\n",
        "        self.time_mask_probability = time_mask_max_percentage\n",
        "        self.probability = probability\n",
        "\n",
        "    def __call__(self, data):\n",
        "        for i in range(len(data['audio'])):\n",
        "            if random.random() < self.probability:\n",
        "                nu, tau = data['audio'][i].shape\n",
        "\n",
        "                f = random.randint(0, int(self.frequency_mask_probability*nu))\n",
        "                f0 = random.randint(0, nu - f)\n",
        "                data['audio'][i][f0:f0 + f, :] = 0\n",
        "\n",
        "                t = random.randint(0, int(self.time_mask_probability*tau))\n",
        "                t0 = random.randint(0, tau - t)\n",
        "                data['audio'][i][:, t0:t0 + t] = 0\n",
        "\n",
        "        return data"
      ],
      "metadata": {
        "id": "H_aZF_DZ0SXE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utils"
      ],
      "metadata": {
        "id": "aGi5Bw537Zh2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fix_seeds(seed=42):\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    torch.backends.cudnn.enabled = False\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "def remove_from_dict(the_dict, keys):\n",
        "    for key in keys:\n",
        "        the_dict.pop(key, None)\n",
        "    return the_dict\n",
        "\n",
        "def prepare_bpe(config):\n",
        "    # train BPE\n",
        "    dataset, ids = get_dataset(config, part='bpe', transforms=TextPreprocess())\n",
        "    train_data_path = 'bpe_texts.txt'\n",
        "    with open(train_data_path, \"w\") as f:\n",
        "        # run ovefr only train part\n",
        "        for i in ids:\n",
        "            text = dataset.get_text(i)\n",
        "            f.write(f\"{text}\\n\")\n",
        "    yttm.BPE.train(data=train_data_path, vocab_size=80, model='yttm.bpe')\n",
        "    os.system(f'rm {train_data_path}')\n",
        "\n",
        "    bpe = yttm.BPE(model='yttm.bpe')\n",
        "    return bpe"
      ],
      "metadata": {
        "id": "0Paa4Z8J7Y_D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "fRLo-GEB0AZ9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encoder"
      ],
      "metadata": {
        "id": "CzDyaCtYLx6L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GroupShuffle для каждого сверточного слоя"
      ],
      "metadata": {
        "id": "nx4BBHl6_HS0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GroupShuffle(nn.Module):\n",
        "\n",
        "    def __init__(self, groups, channels):\n",
        "        super(GroupShuffle, self).__init__()\n",
        "\n",
        "        self.groups = groups\n",
        "        self.channels_per_group = channels // groups\n",
        "\n",
        "    def forward(self, x):\n",
        "        sh = x.shape\n",
        "\n",
        "        x = x.view(-1, self.groups, self.channels_per_group, sh[-1])\n",
        "\n",
        "        x = torch.transpose(x, 1, 2).contiguous()\n",
        "\n",
        "        x = x.view(-1, self.groups * self.channels_per_group, sh[-1])\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "Bl6gKUh39hqc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1D сверточный слой + нормализация"
      ],
      "metadata": {
        "id": "tncZYqUYEqYH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_conv_bn_layer(in_channels, out_channels, kernel_size=11,\n",
        "                     stride=1, dilation=1, padding=0, bias=False,\n",
        "                     groups=1, separable=False,\n",
        "                     normalization=\"batch\", norm_groups=1):\n",
        "    if norm_groups == -1:\n",
        "        norm_groups = out_channels\n",
        "\n",
        "    if separable:\n",
        "        layers = [\n",
        "            nn.Conv1d(in_channels, in_channels, kernel_size,\n",
        "                    stride=stride, dilation=dilation, padding=padding, bias=bias,\n",
        "                    groups=in_channels),\n",
        "            nn.Conv1d(in_channels, out_channels, kernel_size=1,\n",
        "                    stride=1, dilation=1, padding=0, bias=bias, groups=groups)\n",
        "        ]\n",
        "    else:\n",
        "        layers = [\n",
        "            nn.Conv1d(in_channels, out_channels, kernel_size,\n",
        "                    stride=stride, dilation=dilation, padding=padding, bias=bias, groups=groups)\n",
        "        ]\n",
        "\n",
        "    if normalization == \"group\":\n",
        "        layers.append(nn.GroupNorm(\n",
        "            num_groups=norm_groups, num_channels=out_channels))\n",
        "    elif normalization == \"instance\":\n",
        "        layers.append(nn.GroupNorm(\n",
        "            num_groups=out_channels, num_channels=out_channels))\n",
        "    elif normalization == \"layer\":\n",
        "        layers.append(nn.GroupNorm(\n",
        "            num_groups=1, num_channels=out_channels))\n",
        "    elif normalization == \"batch\":\n",
        "        layers.append(nn.BatchNorm1d(out_channels, eps=1e-3, momentum=0.1))\n",
        "    else:\n",
        "        raise ValueError(\n",
        "            f\"Normalization method ({normalization}) does not match\"\n",
        "            f\" one of [batch, layer, group, instance].\")\n",
        "\n",
        "    if groups > 1:\n",
        "        layers.append(GroupShuffle(groups, out_channels))\n",
        "    return nn.Sequential(*layers)"
      ],
      "metadata": {
        "id": "nx5Mq4fv-w7b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Активация + Dropout"
      ],
      "metadata": {
        "id": "8Kl6YJjDFBeB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_act_dropout_layer(drop_prob=0.2, activation='relu'):\n",
        "    if activation is None or activation == 'tanh':\n",
        "        activation = nn.Hardtanh(min_val=0.0, max_val=20.0)\n",
        "    elif activation == 'relu':\n",
        "        activation = nn.ReLU()\n",
        "    layers = [\n",
        "        activation,\n",
        "        nn.Dropout(p=drop_prob)\n",
        "    ]\n",
        "    return nn.Sequential(*layers)"
      ],
      "metadata": {
        "id": "98sb51kWFBqg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Базовый блок c параметром R (repeat) и residual connection"
      ],
      "metadata": {
        "id": "jvPfcekJFWAU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MainBlock(nn.Module):\n",
        "    def __init__(self, inplanes, planes, repeat=3, kernel_size=11, stride=1, residual=True,\n",
        "             dilation=1, dropout=0.2, activation='relu',\n",
        "             groups=1, separable=False, normalization=\"batch\",\n",
        "             norm_groups=1):\n",
        "        super(MainBlock, self).__init__()\n",
        "        padding_val = get_same_padding(kernel_size, stride, dilation)\n",
        "\n",
        "        temp_planes = inplanes\n",
        "        net = []\n",
        "        for _ in range(repeat):\n",
        "            net.append(\n",
        "                get_conv_bn_layer(\n",
        "                    temp_planes,\n",
        "                    planes,\n",
        "                    kernel_size=kernel_size,\n",
        "                    stride=stride,\n",
        "                    dilation=dilation,\n",
        "                    padding=padding_val,\n",
        "                    groups=groups,\n",
        "                    separable=separable,\n",
        "                    normalization=normalization,\n",
        "                    norm_groups=norm_groups)\n",
        "            )\n",
        "            net.append(\n",
        "                get_act_dropout_layer(dropout, activation)\n",
        "            )\n",
        "            temp_planes = planes\n",
        "        self.net = nn.Sequential(*net)\n",
        "        self.residual = residual\n",
        "        if self.residual:\n",
        "            self.residual_layer = get_conv_bn_layer(\n",
        "                                inplanes,\n",
        "                                planes,\n",
        "                                kernel_size=1,\n",
        "                                normalization=normalization,\n",
        "                                norm_groups=norm_groups)\n",
        "        self.out = get_act_dropout_layer(dropout, activation)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.net(x)\n",
        "        if self.residual:\n",
        "            resudial = self.residual_layer(x)\n",
        "            out += resudial\n",
        "        return self.out(out)"
      ],
      "metadata": {
        "id": "m5I1WmXM0mla"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Padding"
      ],
      "metadata": {
        "id": "Zb-Pbam0Hy71"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_same_padding(kernel_size, stride, dilation):\n",
        "    if stride > 1 and dilation > 1:\n",
        "        raise ValueError(\"Only stride OR dilation may be greater than 1\")\n",
        "    if dilation > 1:\n",
        "        return (dilation * kernel_size) // 2 - 1\n",
        "    return kernel_size // 2"
      ],
      "metadata": {
        "id": "0Jib7yOx9LIM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Инициализация весов"
      ],
      "metadata": {
        "id": "-90MJ62YH2Os"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def init_weights(m, mode='xavier_uniform'):\n",
        "    if isinstance(m, nn.Conv1d):\n",
        "        if mode == 'xavier_uniform':\n",
        "            nn.init.xavier_uniform_(m.weight, gain=1.0)\n",
        "        elif mode == 'xavier_normal':\n",
        "            nn.init.xavier_normal_(m.weight, gain=1.0)\n",
        "        elif mode == 'kaiming_uniform':\n",
        "            nn.init.kaiming_uniform_(m.weight, nonlinearity=\"relu\")\n",
        "        elif mode == 'kaiming_normal':\n",
        "            nn.init.kaiming_normal_(m.weight, nonlinearity=\"relu\")\n",
        "        else:\n",
        "            raise ValueError(\"Unknown Initialization mode: {0}\".format(mode))\n",
        "    elif isinstance(m, nn.BatchNorm1d):\n",
        "        if m.track_running_stats:\n",
        "            m.running_mean.zero_()\n",
        "            m.running_var.fill_(1)\n",
        "            m.num_batches_tracked.zero_()\n",
        "        if m.affine:\n",
        "            nn.init.ones_(m.weight)\n",
        "            nn.init.zeros_(m.bias)"
      ],
      "metadata": {
        "id": "77wNJqFuEGok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Сложим все B слоев вместе"
      ],
      "metadata": {
        "id": "gEHjE8R_H52f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class QuartzNet(nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            model_config,\n",
        "            feat_in,\n",
        "            vocab_size,\n",
        "            activation='relu',\n",
        "            normalization_mode=\"batch\",\n",
        "            norm_groups=-1,\n",
        "            frame_splicing=1,\n",
        "            init_mode='xavier_uniform',\n",
        "            **kwargs\n",
        "    ):\n",
        "        super(QuartzNet, self).__init__()\n",
        "\n",
        "        feat_in = feat_in * frame_splicing\n",
        "        self.stride = 1\n",
        "\n",
        "        residual_panes = []\n",
        "        layers = []\n",
        "        for lcfg in model_config:\n",
        "            self.stride *= lcfg['stride']\n",
        "\n",
        "            groups = lcfg.get('groups', 1)\n",
        "            separable = lcfg.get('separable', False)\n",
        "            residual = lcfg.get('residual', True)\n",
        "            layers.append(\n",
        "                MainBlock(feat_in,\n",
        "                    lcfg['filters'],\n",
        "                    repeat=lcfg['repeat'],\n",
        "                    kernel_size=lcfg['kernel'],\n",
        "                    stride=lcfg['stride'],\n",
        "                    dilation=lcfg['dilation'],\n",
        "                    dropout=lcfg['dropout'] if 'dropout' in lcfg else 0.0,\n",
        "                    residual=residual,\n",
        "                    groups=groups,\n",
        "                    separable=separable,\n",
        "                    normalization=normalization_mode,\n",
        "                    norm_groups=norm_groups,\n",
        "                    activation=activation))\n",
        "            feat_in = lcfg['filters']\n",
        "\n",
        "        self.encoder = nn.Sequential(*layers)\n",
        "        self.classify = nn.Conv1d(1024, vocab_size,\n",
        "                      kernel_size=1, bias=True)\n",
        "        self.apply(lambda x: init_weights(x, mode=init_mode))\n",
        "\n",
        "    def forward(self, audio_signal):\n",
        "        feat = self.encoder(audio_signal)\n",
        "        # BxCxT\n",
        "        return self.classify(feat)\n",
        "\n",
        "    def load_weights(self, path, map_location='cpu'):\n",
        "        weights = torch.load(path, map_location=map_location)\n",
        "        print(self.load_state_dict(weights, strict=False))"
      ],
      "metadata": {
        "id": "0KtCEVC3886L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_quartznet5x5_config = [\n",
        "    {'filters': 256, 'repeat': 1, 'kernel': 33, 'stride': 2, 'dilation': 1, 'dropout': 0.2, 'residual': False, 'separable': True},\n",
        "\n",
        "    {'filters': 256, 'repeat': 5, 'kernel': 33, 'stride': 1, 'dilation': 1, 'dropout': 0.2, 'residual': True, 'separable': True},\n",
        "\n",
        "    {'filters': 256, 'repeat': 5, 'kernel': 39, 'stride': 1, 'dilation': 1, 'dropout': 0.2, 'residual': True, 'separable': True},\n",
        "\n",
        "    {'filters': 512, 'repeat': 5, 'kernel': 51, 'stride': 1, 'dilation': 1, 'dropout': 0.2, 'residual': True, 'separable': True},\n",
        "\n",
        "    {'filters': 512, 'repeat': 5, 'kernel': 63, 'stride': 1, 'dilation': 1, 'dropout': 0.2, 'residual': True, 'separable': True},\n",
        "\n",
        "    {'filters': 512, 'repeat': 5, 'kernel': 75, 'stride': 1, 'dilation': 1, 'dropout': 0.2, 'residual': True, 'separable': True},\n",
        "\n",
        "    {'filters': 512, 'repeat': 1, 'kernel': 87, 'stride': 1, 'dilation': 2, 'dropout': 0.2, 'residual': False, 'separable': True},\n",
        "\n",
        "    {'filters': 1024, 'repeat': 1, 'kernel': 1, 'stride': 1, 'dilation': 1, 'dropout': 0.2, 'residual': False, 'separable': False}\n",
        "]"
      ],
      "metadata": {
        "id": "hk1wOwxHIZSP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scratch = QuartzNet(_quartznet5x5_config, 64, 28)"
      ],
      "metadata": {
        "id": "gmbLF-c8IZ3o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decoder"
      ],
      "metadata": {
        "id": "E4SDzKEWL12n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(object):\n",
        "    \"\"\"\n",
        "    Basic decoder class from which all other decoders inherit. Implements several\n",
        "    helper functions. Subclasses should implement the decode() method.\n",
        "    Arguments:\n",
        "        labels (string): mapping from integers to characters.\n",
        "        blank_index (int, optional): index for the blank '_' character. Defaults to 0.\n",
        "        space_index (int, optional): index for the space ' ' character. Defaults to 28.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, bpe, blank_index=0, space_simbol='▁'):\n",
        "        # e.g. labels = \"_'ABCDEFGHIJKLMNOPQRSTUVWXYZ#\"\n",
        "        self.labels = labels = bpe.vocab()\n",
        "        print(labels)\n",
        "        self.int_to_char = bpe.id_to_subword\n",
        "        self.blank_index = blank_index\n",
        "        self.space_simbol = space_simbol\n",
        "        space_index = None  # To prevent errors in decode, we add an out of bounds index for the space\n",
        "        if self.space_simbol in labels:\n",
        "            space_index = labels.index(self.space_simbol)\n",
        "        else:\n",
        "            raise ValueError('I wanna break free!!!')\n",
        "        self.space_index = space_index\n",
        "\n",
        "    def wer(self, s1, s2):\n",
        "        \"\"\"\n",
        "        Computes the Word Error Rate, defined as the edit distance between the\n",
        "        two provided sentences after tokenizing to words.\n",
        "        Arguments:\n",
        "            s1 (string): space-separated sentence\n",
        "            s2 (string): space-separated sentence\n",
        "        \"\"\"\n",
        "\n",
        "        # build mapping of words to integers\n",
        "        b = set(s1.split(self.space_simbol) + s2.split(self.space_simbol))\n",
        "        word2char = dict(zip(b, range(len(b))))\n",
        "\n",
        "        # map the words to a char array (Levenshtein packages only accepts\n",
        "        # strings)\n",
        "        w1 = [chr(word2char[w]) for w in s1.split(self.space_simbol)]\n",
        "        w2 = [chr(word2char[w]) for w in s2.split(self.space_simbol)]\n",
        "\n",
        "        return Lev.distance(''.join(w1), ''.join(w2)) / len(w1)\n",
        "\n",
        "    def cer(self, s1, s2):\n",
        "        \"\"\"\n",
        "        Computes the Character Error Rate, defined as the edit distance.\n",
        "        Arguments:\n",
        "            s1 (string): space-separated sentence\n",
        "            s2 (string): space-separated sentence\n",
        "        \"\"\"\n",
        "        s1_concated, s2_concated = s1.replace(self.space_simbol, ''), s2.replace(self.space_simbol, '')\n",
        "        return Lev.distance(s1_concated, s2_concated) / len(s1)\n",
        "\n",
        "    def convert_to_strings(self, sequences, sizes=None, remove_repetitions=False, return_offsets=False):\n",
        "        \"\"\"Given a list of numeric sequences, returns the corresponding strings\"\"\"\n",
        "        strings = []\n",
        "        offsets = [] if return_offsets else None\n",
        "        for x in range(len(sequences)):\n",
        "            seq_len = sizes[x] if sizes is not None else len(sequences[x])\n",
        "            string, string_offsets = self.process_string(sequences[x], seq_len, remove_repetitions)\n",
        "            strings.append(string)  # We only return one path\n",
        "            if return_offsets:\n",
        "                offsets.append(string_offsets)\n",
        "        if return_offsets:\n",
        "            return strings, offsets\n",
        "        else:\n",
        "            return strings\n",
        "\n",
        "    def process_string(self, sequence, size, remove_repetitions=False):\n",
        "        string = ''\n",
        "        offsets = []\n",
        "        for i in range(size):\n",
        "            char = self.int_to_char(sequence[i].item())\n",
        "            if char != self.int_to_char(self.blank_index):\n",
        "                # if this char is a repetition and remove_repetitions=true, then skip\n",
        "                if remove_repetitions and i != 0 and char == self.int_to_char(sequence[i - 1].item()):\n",
        "                    pass\n",
        "                elif char == self.labels[self.space_index]:\n",
        "                    string += self.space_simbol\n",
        "                    offsets.append(i)\n",
        "                else:\n",
        "                    string = string + char\n",
        "                    offsets.append(i)\n",
        "        return string, torch.tensor(offsets, dtype=torch.int)\n",
        "\n",
        "    def decode(self, probs, sizes=None):\n",
        "        \"\"\"\n",
        "        Given a matrix of character probabilities, returns the decoder's\n",
        "        best guess of the transcription\n",
        "        Arguments:\n",
        "            probs: Tensor of character probabilities, where probs[c,t]\n",
        "                            is the probability of character c at time t\n",
        "            sizes(optional): Size of each sequence in the mini-batch\n",
        "        Returns:\n",
        "            string: sequence of the model's best guess for the transcription\n",
        "        \"\"\"\n",
        "        raise NotImplementedError\n",
        "\n",
        "\n",
        "class BeamCTCDecoder(Decoder):\n",
        "    def __init__(self, bpe, lm_path=None, alpha=0, beta=0, cutoff_top_n=40, cutoff_prob=1.0, beam_width=100,\n",
        "                 num_processes=4, blank_index=0):\n",
        "        self.labels = labels = bpe.vocab()\n",
        "        super(BeamCTCDecoder, self).__init__(bpe=bpe)\n",
        "        try:\n",
        "            from ctcdecode import CTCBeamDecoder\n",
        "        except ImportError:\n",
        "            raise ImportError(\"BeamCTCDecoder requires paddledecoder package.\")\n",
        "        self._decoder = CTCBeamDecoder(labels, \n",
        "                model_path=lm_path, alpha=alpha, beta=beta, cutoff_top_n=cutoff_top_n, \n",
        "                cutoff_prob=cutoff_prob, beam_width=beam_width, num_processes=num_processes, blank_id=self.blank_index)\n",
        "\n",
        "    def convert_to_strings_ctc(self, out, seq_len):\n",
        "        results = []\n",
        "        for b, batch in enumerate(out):\n",
        "            utterances = []\n",
        "            for p, utt in enumerate(batch):\n",
        "                size = seq_len[b][p]\n",
        "                if size > 0:\n",
        "                    transcript = ''.join(map(lambda x: self.int_to_char(x.item()), utt[0:size]))\n",
        "                else:\n",
        "                    transcript = ''\n",
        "                utterances.append(transcript)\n",
        "            results.append(utterances)\n",
        "        return results\n",
        "\n",
        "    def convert_tensor_ctc(self, offsets, sizes):\n",
        "        results = []\n",
        "        for b, batch in enumerate(offsets):\n",
        "            utterances = []\n",
        "            for p, utt in enumerate(batch):\n",
        "                size = sizes[b][p]\n",
        "                if sizes[b][p] > 0:\n",
        "                    utterances.append(utt[0:size])\n",
        "                else:\n",
        "                    utterances.append(torch.tensor([], dtype=torch.int))\n",
        "            results.append(utterances)\n",
        "        return results\n",
        "\n",
        "    def decode(self, probs, sizes=None):\n",
        "        \"\"\"\n",
        "        Decodes probability output using ctcdecode package.\n",
        "        Arguments:\n",
        "            probs: Tensor of character probabilities, where probs[c,t]\n",
        "                            is the probability of character c at time t\n",
        "            sizes: Size of each sequence in the mini-batch\n",
        "        Returns:\n",
        "            string: sequences of the model's best guess for the transcription\n",
        "        \"\"\"\n",
        "        probs = probs.cpu()\n",
        "        out, scores, offsets, seq_lens = self._decoder.decode(probs, sizes)\n",
        "        # print(scores)\n",
        "        strings = self.convert_to_strings_ctc(out, seq_lens)\n",
        "        strings = [item[0] for item in strings]\n",
        "        # offsets = self.convert_tensor_ctc(offsets, seq_lens)\n",
        "        return strings\n",
        "\n",
        "\n",
        "class GreedyDecoder(Decoder):\n",
        "    def decode(self, probs, sizes=None):\n",
        "        \"\"\"\n",
        "        Returns the argmax decoding given the probability matrix. Removes\n",
        "        repeated elements in the sequence, as well as blanks.\n",
        "        Arguments:\n",
        "            probs: Tensor of character probabilities from the network. Expected shape of batch x seq_length x output_dim\n",
        "            sizes(optional): Size of each sequence in the mini-batch\n",
        "        Returns:\n",
        "            strings: sequences of the model's best guess for the transcription on inputs\n",
        "            offsets: time step per character predicted\n",
        "        \"\"\"\n",
        "        _, max_probs = torch.max(probs, 2)\n",
        "        strings, offsets = self.convert_to_strings(max_probs.view(max_probs.size(0), max_probs.size(1)), sizes,\n",
        "                                                   remove_repetitions=True, return_offsets=True)\n",
        "        return strings"
      ],
      "metadata": {
        "id": "vrtjOLDhL3Wd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train"
      ],
      "metadata": {
        "id": "0Ym9nOy-64g_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fix_seeds(42)\n",
        "config = None\n",
        "bpe = prepare_bpe(config)"
      ],
      "metadata": {
        "id": "DU6zA_qAB2xD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(config):\n",
        "    fix_seeds(seed=42)\n",
        "    config = None\n",
        "    num_epoches=2\n",
        "    bpe = prepare_bpe(config)\n",
        "\n",
        "    transforms_train = Compose([\n",
        "            TextPreprocess(),\n",
        "            ToNumpy(),\n",
        "            BPEtexts(bpe=bpe, dropout_prob=0.05),\n",
        "            AudioSqueeze(),\n",
        "            AddGaussianNoise(\n",
        "                min_amplitude=0.001,\n",
        "                max_amplitude=0.015,\n",
        "                p=0.5\n",
        "            ),\n",
        "            TimeStretch(\n",
        "                min_rate=0.8,\n",
        "                max_rate=1.25,\n",
        "                p=0.5\n",
        "            ),\n",
        "            PitchShift(\n",
        "                min_semitones=-4,\n",
        "                max_semitones=4,\n",
        "                p=0.5\n",
        "            )\n",
        "    ])\n",
        "\n",
        "    batch_transforms_train = Compose([\n",
        "            ToGpu('cuda' if torch.cuda.is_available() else 'cpu'),\n",
        "            NormalizedMelSpectrogram(\n",
        "                sample_rate=16000,\n",
        "                n_mels=64,\n",
        "                normalize='touniform'\n",
        "            ).to('cuda' if torch.cuda.is_available() else 'cpu'),\n",
        "            MaskSpectrogram(\n",
        "                probability=0.5,\n",
        "                time_mask_max_percentage=0.05,\n",
        "                frequency_mask_max_percentage=0.15\n",
        "            ),\n",
        "            AddLengths(),\n",
        "            Pad()\n",
        "    ])\n",
        "\n",
        "    transforms_val = Compose([\n",
        "            TextPreprocess(),\n",
        "            ToNumpy(),\n",
        "            BPEtexts(bpe=bpe),\n",
        "            AudioSqueeze()\n",
        "    ])\n",
        "\n",
        "    batch_transforms_val = Compose([\n",
        "            ToGpu('cuda' if torch.cuda.is_available() else 'cpu'),\n",
        "            NormalizedMelSpectrogram(\n",
        "                sample_rate=16000,\n",
        "                n_mels=64,\n",
        "                normalize='touniform'\n",
        "            ).to('cuda' if torch.cuda.is_available() else 'cpu'),\n",
        "            AddLengths(),\n",
        "            Pad()\n",
        "    ])\n",
        "\n",
        "    # load datasets\n",
        "    train_dataset = get_dataset(config, transforms=transforms_train, part='train')\n",
        "    val_dataset = get_dataset(config, transforms=transforms_val, part='val')\n",
        "\n",
        "    #take subsets\n",
        "    indices_train = list(range(0, len(train_dataset), 50))\n",
        "    trainsubset = torch.utils.data.Subset(train_dataset, indices_train)\n",
        "    indices_val = list(range(0, len(val_dataset), 50))\n",
        "    valsubset = torch.utils.data.Subset(val_dataset, indices_val)\n",
        "\n",
        "    train_dataloader = DataLoader(trainsubset, num_workers=16,\n",
        "                batch_size=32, collate_fn=no_pad_collate)\n",
        "\n",
        "    val_dataloader = DataLoader(valsubset, num_workers=16,\n",
        "                batch_size=1, collate_fn=no_pad_collate)\n",
        "\n",
        "    model = QuartzNet(_quartznet5x5_config, 64, 80)\n",
        "\n",
        "    print(model)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0005, weight_decay=0.0001)\n",
        "    num_steps = len(train_dataloader) * num_epoches\n",
        "    print('num steps:', num_steps)\n",
        "    lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_steps)\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        model = model.cuda()\n",
        "\n",
        "    criterion = nn.CTCLoss(blank=0, reduction='mean', zero_infinity=True)\n",
        "    decoder = GreedyDecoder(bpe=bpe)\n",
        "\n",
        "    prev_wer = 1000\n",
        "    for epoch_idx in tqdm(range(num_epoches)):\n",
        "        # train:\n",
        "        model.train()\n",
        "        for batch_idx, batch in enumerate(train_dataloader):\n",
        "            batch = batch_transforms_train(batch)\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(batch['audio'])\n",
        "            output_length = torch.ceil(batch['input_lengths'].float() / model.stride).int()\n",
        "            loss = criterion(logits.permute(2, 0, 1).log_softmax(dim=2), batch['text'], output_length, batch['target_lengths'])\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 15)\n",
        "            optimizer.step()\n",
        "            lr_scheduler.step()\n",
        "\n",
        "            if batch_idx % 20 == 0:\n",
        "                target_strings = decoder.convert_to_strings(batch['text'])\n",
        "                decoded_output = decoder.decode(logits.permute(0, 2, 1).softmax(dim=2))\n",
        "\n",
        "                wer = np.mean([decoder.wer(true, pred) for true, pred in zip(target_strings, decoded_output)])\n",
        "                cer = np.mean([decoder.cer(true, pred) for true, pred in zip(target_strings, decoded_output)])\n",
        "                step = epoch_idx * len(train_dataloader) * train_dataloader.batch_size + batch_idx * train_dataloader.batch_size\n",
        "\n",
        "        # validate:\n",
        "        model.eval()\n",
        "        val_stats = defaultdict(list)\n",
        "        for batch_idx, batch in enumerate(val_dataloader):\n",
        "            batch = batch_transforms_val(batch)\n",
        "            with torch.no_grad():\n",
        "                logits = model(batch['audio'])\n",
        "                output_length = torch.ceil(batch['input_lengths'].float() / model.stride).int()\n",
        "                loss = criterion(logits.permute(2, 0, 1).log_softmax(dim=2), batch['text'], output_length, batch['target_lengths'])\n",
        "\n",
        "            target_strings = decoder.convert_to_strings(batch['text'])\n",
        "            decoded_output = decoder.decode(logits.permute(0, 2, 1).softmax(dim=2))\n",
        "            wer = np.mean([decoder.wer(true, pred) for true, pred in zip(target_strings, decoded_output)])\n",
        "            cer = np.mean([decoder.cer(true, pred) for true, pred in zip(target_strings, decoded_output)])\n",
        "            val_stats['val_loss'].append(loss.item())\n",
        "            val_stats['wer'].append(wer)\n",
        "            val_stats['cer'].append(cer)\n",
        "        for k, v in val_stats.items():\n",
        "            val_stats[k] = np.mean(v)\n",
        "\n",
        "        # save model, TODO: save optimizer:\n",
        "        if val_stats['wer'] < prev_wer:\n",
        "            os.makedirs('logs', exist_ok=True)\n",
        "            prev_wer = val_stats['wer']\n",
        "            torch.save(\n",
        "                model.state_dict(),\n",
        "                os.path.join('logs', f'model_{epoch_idx}_{prev_wer}.pth')\n",
        "            )"
      ],
      "metadata": {
        "id": "hO5mPieJJ9Se"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(None)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JnldqLP5NJvZ",
        "outputId": "abe3d420-b1c3-4338-d4ce-c4037f473da5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:490: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "QuartzNet(\n",
            "  (encoder): Sequential(\n",
            "    (0): MainBlock(\n",
            "      (net): Sequential(\n",
            "        (0): Sequential(\n",
            "          (0): Conv1d(64, 64, kernel_size=(33,), stride=(2,), padding=(16,), groups=64, bias=False)\n",
            "          (1): Conv1d(64, 256, kernel_size=(1,), stride=(1,), bias=False)\n",
            "          (2): BatchNorm1d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (1): Sequential(\n",
            "          (0): ReLU()\n",
            "          (1): Dropout(p=0.2, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (out): Sequential(\n",
            "        (0): ReLU()\n",
            "        (1): Dropout(p=0.2, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (1): MainBlock(\n",
            "      (net): Sequential(\n",
            "        (0): Sequential(\n",
            "          (0): Conv1d(256, 256, kernel_size=(33,), stride=(1,), padding=(16,), groups=256, bias=False)\n",
            "          (1): Conv1d(256, 256, kernel_size=(1,), stride=(1,), bias=False)\n",
            "          (2): BatchNorm1d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (1): Sequential(\n",
            "          (0): ReLU()\n",
            "          (1): Dropout(p=0.2, inplace=False)\n",
            "        )\n",
            "        (2): Sequential(\n",
            "          (0): Conv1d(256, 256, kernel_size=(33,), stride=(1,), padding=(16,), groups=256, bias=False)\n",
            "          (1): Conv1d(256, 256, kernel_size=(1,), stride=(1,), bias=False)\n",
            "          (2): BatchNorm1d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (3): Sequential(\n",
            "          (0): ReLU()\n",
            "          (1): Dropout(p=0.2, inplace=False)\n",
            "        )\n",
            "        (4): Sequential(\n",
            "          (0): Conv1d(256, 256, kernel_size=(33,), stride=(1,), padding=(16,), groups=256, bias=False)\n",
            "          (1): Conv1d(256, 256, kernel_size=(1,), stride=(1,), bias=False)\n",
            "          (2): BatchNorm1d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (5): Sequential(\n",
            "          (0): ReLU()\n",
            "          (1): Dropout(p=0.2, inplace=False)\n",
            "        )\n",
            "        (6): Sequential(\n",
            "          (0): Conv1d(256, 256, kernel_size=(33,), stride=(1,), padding=(16,), groups=256, bias=False)\n",
            "          (1): Conv1d(256, 256, kernel_size=(1,), stride=(1,), bias=False)\n",
            "          (2): BatchNorm1d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (7): Sequential(\n",
            "          (0): ReLU()\n",
            "          (1): Dropout(p=0.2, inplace=False)\n",
            "        )\n",
            "        (8): Sequential(\n",
            "          (0): Conv1d(256, 256, kernel_size=(33,), stride=(1,), padding=(16,), groups=256, bias=False)\n",
            "          (1): Conv1d(256, 256, kernel_size=(1,), stride=(1,), bias=False)\n",
            "          (2): BatchNorm1d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (9): Sequential(\n",
            "          (0): ReLU()\n",
            "          (1): Dropout(p=0.2, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (residual_layer): Sequential(\n",
            "        (0): Conv1d(256, 256, kernel_size=(1,), stride=(1,), bias=False)\n",
            "        (1): BatchNorm1d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (out): Sequential(\n",
            "        (0): ReLU()\n",
            "        (1): Dropout(p=0.2, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (2): MainBlock(\n",
            "      (net): Sequential(\n",
            "        (0): Sequential(\n",
            "          (0): Conv1d(256, 256, kernel_size=(39,), stride=(1,), padding=(19,), groups=256, bias=False)\n",
            "          (1): Conv1d(256, 256, kernel_size=(1,), stride=(1,), bias=False)\n",
            "          (2): BatchNorm1d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (1): Sequential(\n",
            "          (0): ReLU()\n",
            "          (1): Dropout(p=0.2, inplace=False)\n",
            "        )\n",
            "        (2): Sequential(\n",
            "          (0): Conv1d(256, 256, kernel_size=(39,), stride=(1,), padding=(19,), groups=256, bias=False)\n",
            "          (1): Conv1d(256, 256, kernel_size=(1,), stride=(1,), bias=False)\n",
            "          (2): BatchNorm1d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (3): Sequential(\n",
            "          (0): ReLU()\n",
            "          (1): Dropout(p=0.2, inplace=False)\n",
            "        )\n",
            "        (4): Sequential(\n",
            "          (0): Conv1d(256, 256, kernel_size=(39,), stride=(1,), padding=(19,), groups=256, bias=False)\n",
            "          (1): Conv1d(256, 256, kernel_size=(1,), stride=(1,), bias=False)\n",
            "          (2): BatchNorm1d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (5): Sequential(\n",
            "          (0): ReLU()\n",
            "          (1): Dropout(p=0.2, inplace=False)\n",
            "        )\n",
            "        (6): Sequential(\n",
            "          (0): Conv1d(256, 256, kernel_size=(39,), stride=(1,), padding=(19,), groups=256, bias=False)\n",
            "          (1): Conv1d(256, 256, kernel_size=(1,), stride=(1,), bias=False)\n",
            "          (2): BatchNorm1d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (7): Sequential(\n",
            "          (0): ReLU()\n",
            "          (1): Dropout(p=0.2, inplace=False)\n",
            "        )\n",
            "        (8): Sequential(\n",
            "          (0): Conv1d(256, 256, kernel_size=(39,), stride=(1,), padding=(19,), groups=256, bias=False)\n",
            "          (1): Conv1d(256, 256, kernel_size=(1,), stride=(1,), bias=False)\n",
            "          (2): BatchNorm1d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (9): Sequential(\n",
            "          (0): ReLU()\n",
            "          (1): Dropout(p=0.2, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (residual_layer): Sequential(\n",
            "        (0): Conv1d(256, 256, kernel_size=(1,), stride=(1,), bias=False)\n",
            "        (1): BatchNorm1d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (out): Sequential(\n",
            "        (0): ReLU()\n",
            "        (1): Dropout(p=0.2, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (3): MainBlock(\n",
            "      (net): Sequential(\n",
            "        (0): Sequential(\n",
            "          (0): Conv1d(256, 256, kernel_size=(51,), stride=(1,), padding=(25,), groups=256, bias=False)\n",
            "          (1): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)\n",
            "          (2): BatchNorm1d(512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (1): Sequential(\n",
            "          (0): ReLU()\n",
            "          (1): Dropout(p=0.2, inplace=False)\n",
            "        )\n",
            "        (2): Sequential(\n",
            "          (0): Conv1d(512, 512, kernel_size=(51,), stride=(1,), padding=(25,), groups=512, bias=False)\n",
            "          (1): Conv1d(512, 512, kernel_size=(1,), stride=(1,), bias=False)\n",
            "          (2): BatchNorm1d(512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (3): Sequential(\n",
            "          (0): ReLU()\n",
            "          (1): Dropout(p=0.2, inplace=False)\n",
            "        )\n",
            "        (4): Sequential(\n",
            "          (0): Conv1d(512, 512, kernel_size=(51,), stride=(1,), padding=(25,), groups=512, bias=False)\n",
            "          (1): Conv1d(512, 512, kernel_size=(1,), stride=(1,), bias=False)\n",
            "          (2): BatchNorm1d(512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (5): Sequential(\n",
            "          (0): ReLU()\n",
            "          (1): Dropout(p=0.2, inplace=False)\n",
            "        )\n",
            "        (6): Sequential(\n",
            "          (0): Conv1d(512, 512, kernel_size=(51,), stride=(1,), padding=(25,), groups=512, bias=False)\n",
            "          (1): Conv1d(512, 512, kernel_size=(1,), stride=(1,), bias=False)\n",
            "          (2): BatchNorm1d(512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (7): Sequential(\n",
            "          (0): ReLU()\n",
            "          (1): Dropout(p=0.2, inplace=False)\n",
            "        )\n",
            "        (8): Sequential(\n",
            "          (0): Conv1d(512, 512, kernel_size=(51,), stride=(1,), padding=(25,), groups=512, bias=False)\n",
            "          (1): Conv1d(512, 512, kernel_size=(1,), stride=(1,), bias=False)\n",
            "          (2): BatchNorm1d(512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (9): Sequential(\n",
            "          (0): ReLU()\n",
            "          (1): Dropout(p=0.2, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (residual_layer): Sequential(\n",
            "        (0): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)\n",
            "        (1): BatchNorm1d(512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (out): Sequential(\n",
            "        (0): ReLU()\n",
            "        (1): Dropout(p=0.2, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (4): MainBlock(\n",
            "      (net): Sequential(\n",
            "        (0): Sequential(\n",
            "          (0): Conv1d(512, 512, kernel_size=(63,), stride=(1,), padding=(31,), groups=512, bias=False)\n",
            "          (1): Conv1d(512, 512, kernel_size=(1,), stride=(1,), bias=False)\n",
            "          (2): BatchNorm1d(512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (1): Sequential(\n",
            "          (0): ReLU()\n",
            "          (1): Dropout(p=0.2, inplace=False)\n",
            "        )\n",
            "        (2): Sequential(\n",
            "          (0): Conv1d(512, 512, kernel_size=(63,), stride=(1,), padding=(31,), groups=512, bias=False)\n",
            "          (1): Conv1d(512, 512, kernel_size=(1,), stride=(1,), bias=False)\n",
            "          (2): BatchNorm1d(512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (3): Sequential(\n",
            "          (0): ReLU()\n",
            "          (1): Dropout(p=0.2, inplace=False)\n",
            "        )\n",
            "        (4): Sequential(\n",
            "          (0): Conv1d(512, 512, kernel_size=(63,), stride=(1,), padding=(31,), groups=512, bias=False)\n",
            "          (1): Conv1d(512, 512, kernel_size=(1,), stride=(1,), bias=False)\n",
            "          (2): BatchNorm1d(512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (5): Sequential(\n",
            "          (0): ReLU()\n",
            "          (1): Dropout(p=0.2, inplace=False)\n",
            "        )\n",
            "        (6): Sequential(\n",
            "          (0): Conv1d(512, 512, kernel_size=(63,), stride=(1,), padding=(31,), groups=512, bias=False)\n",
            "          (1): Conv1d(512, 512, kernel_size=(1,), stride=(1,), bias=False)\n",
            "          (2): BatchNorm1d(512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (7): Sequential(\n",
            "          (0): ReLU()\n",
            "          (1): Dropout(p=0.2, inplace=False)\n",
            "        )\n",
            "        (8): Sequential(\n",
            "          (0): Conv1d(512, 512, kernel_size=(63,), stride=(1,), padding=(31,), groups=512, bias=False)\n",
            "          (1): Conv1d(512, 512, kernel_size=(1,), stride=(1,), bias=False)\n",
            "          (2): BatchNorm1d(512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (9): Sequential(\n",
            "          (0): ReLU()\n",
            "          (1): Dropout(p=0.2, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (residual_layer): Sequential(\n",
            "        (0): Conv1d(512, 512, kernel_size=(1,), stride=(1,), bias=False)\n",
            "        (1): BatchNorm1d(512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (out): Sequential(\n",
            "        (0): ReLU()\n",
            "        (1): Dropout(p=0.2, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (5): MainBlock(\n",
            "      (net): Sequential(\n",
            "        (0): Sequential(\n",
            "          (0): Conv1d(512, 512, kernel_size=(75,), stride=(1,), padding=(37,), groups=512, bias=False)\n",
            "          (1): Conv1d(512, 512, kernel_size=(1,), stride=(1,), bias=False)\n",
            "          (2): BatchNorm1d(512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (1): Sequential(\n",
            "          (0): ReLU()\n",
            "          (1): Dropout(p=0.2, inplace=False)\n",
            "        )\n",
            "        (2): Sequential(\n",
            "          (0): Conv1d(512, 512, kernel_size=(75,), stride=(1,), padding=(37,), groups=512, bias=False)\n",
            "          (1): Conv1d(512, 512, kernel_size=(1,), stride=(1,), bias=False)\n",
            "          (2): BatchNorm1d(512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (3): Sequential(\n",
            "          (0): ReLU()\n",
            "          (1): Dropout(p=0.2, inplace=False)\n",
            "        )\n",
            "        (4): Sequential(\n",
            "          (0): Conv1d(512, 512, kernel_size=(75,), stride=(1,), padding=(37,), groups=512, bias=False)\n",
            "          (1): Conv1d(512, 512, kernel_size=(1,), stride=(1,), bias=False)\n",
            "          (2): BatchNorm1d(512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (5): Sequential(\n",
            "          (0): ReLU()\n",
            "          (1): Dropout(p=0.2, inplace=False)\n",
            "        )\n",
            "        (6): Sequential(\n",
            "          (0): Conv1d(512, 512, kernel_size=(75,), stride=(1,), padding=(37,), groups=512, bias=False)\n",
            "          (1): Conv1d(512, 512, kernel_size=(1,), stride=(1,), bias=False)\n",
            "          (2): BatchNorm1d(512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (7): Sequential(\n",
            "          (0): ReLU()\n",
            "          (1): Dropout(p=0.2, inplace=False)\n",
            "        )\n",
            "        (8): Sequential(\n",
            "          (0): Conv1d(512, 512, kernel_size=(75,), stride=(1,), padding=(37,), groups=512, bias=False)\n",
            "          (1): Conv1d(512, 512, kernel_size=(1,), stride=(1,), bias=False)\n",
            "          (2): BatchNorm1d(512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (9): Sequential(\n",
            "          (0): ReLU()\n",
            "          (1): Dropout(p=0.2, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (residual_layer): Sequential(\n",
            "        (0): Conv1d(512, 512, kernel_size=(1,), stride=(1,), bias=False)\n",
            "        (1): BatchNorm1d(512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (out): Sequential(\n",
            "        (0): ReLU()\n",
            "        (1): Dropout(p=0.2, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (6): MainBlock(\n",
            "      (net): Sequential(\n",
            "        (0): Sequential(\n",
            "          (0): Conv1d(512, 512, kernel_size=(87,), stride=(1,), padding=(86,), dilation=(2,), groups=512, bias=False)\n",
            "          (1): Conv1d(512, 512, kernel_size=(1,), stride=(1,), bias=False)\n",
            "          (2): BatchNorm1d(512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (1): Sequential(\n",
            "          (0): ReLU()\n",
            "          (1): Dropout(p=0.2, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (out): Sequential(\n",
            "        (0): ReLU()\n",
            "        (1): Dropout(p=0.2, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (7): MainBlock(\n",
            "      (net): Sequential(\n",
            "        (0): Sequential(\n",
            "          (0): Conv1d(512, 1024, kernel_size=(1,), stride=(1,), bias=False)\n",
            "          (1): BatchNorm1d(1024, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (1): Sequential(\n",
            "          (0): ReLU()\n",
            "          (1): Dropout(p=0.2, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (out): Sequential(\n",
            "        (0): ReLU()\n",
            "        (1): Dropout(p=0.2, inplace=False)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (classify): Conv1d(1024, 80, kernel_size=(1,), stride=(1,))\n",
            ")\n",
            "num steps: 36\n",
            "['<PAD>', '<UNK>', '<BOS>', '<EOS>', '▁', 'e', 't', 'a', 'o', 'n', 'i', 'h', 's', 'r', 'd', 'l', 'u', 'm', 'c', 'w', 'f', 'g', 'y', 'p', 'b', 'v', 'k', 'x', 'j', 'q', 'z', '▁t', 'he', '▁a', '▁the', 'in', '▁w', '▁s', '▁o', 're', 'nd', '▁h', '▁b', 'er', '▁m', 'ou', '▁i', '▁c', 'ed', '▁f', 'at', 'en', '▁and', '▁to', '▁of', 'on', 'is', 'ing', '▁p', '▁th', '▁d', 'or', '▁he', 'es', 'as', '▁l', '▁in', 'ar', 'an', 'it', 'll', '▁n', '▁g', 'om', '▁ha', '▁be', '▁e', 'le', 'ic', 'ot']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2/2 [03:47<00:00, 113.78s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "nA7Eesnlgrdq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}